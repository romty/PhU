# -*- coding: utf-8 -*-
"""Phase.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13zkFiuzPUYbwT7J56hJtl_RYwQR4m99L
"""

from tensorflow.keras.utils import to_categorical ,Sequence
import scipy.io
import numpy as np
from sklearn import preprocessing
import cmath 
import model
from unwrap import unwrap


def normalize_real (x_source):
    a_oo = x_source - x_source.real.min() - 1j*x_source.imag.min() # origin offsetted
    return a_oo/np.abs(a_oo).max()
    
def normalize_angle (img):
  ximg =(img-np.min(img))/(np.max(img)-np.min(img))
  return ximg
  
  
class DataGenerator(Sequence):
    'Generates data for Keras'
    
    def __init__(self, pair, class_map, batch_size=16, dim=(256,256,1), shuffle=True):
        'Initialization'
        self.dim = dim
        self.pair = pair
        self.class_map = class_map
        self.batch_size = batch_size
        self.shuffle = shuffle
        self.on_epoch_end()

    def __len__(self):
        'Denotes the number of batches per epoch'
        return int(np.floor(len(self.pair) / self.batch_size))

    def __getitem__(self, index):
        'Generate one batch of data'
        # Generate indexes of the batch
        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]

        # Find list of IDs
        list_IDs_temp = [k for k in indexes]

        # Generate data
        X, y = self.__data_generation(list_IDs_temp)

        return X, y

    def on_epoch_end(self):
        'Updates indexes after each epoch'
        self.indexes = np.arange(len(self.pair))
        if self.shuffle == True:
            np.random.shuffle(self.indexes)

    def __data_generation(self, list_IDs_temp):
        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)
        # Initialization
        batch_imgs = list()
        batch_labels = list()

        # Generate data
        for i in list_IDs_temp:
            # Store sample
            #print (self.pair[i][0])
            img = scipy.io.loadmat(self.pair[i][0])['wrap']
            img_normalized = normalize_angle(np.angle(img))
            batch_imgs.append(np.expand_dims(img_normalized, axis=-1))

            label =unwrap(np.angle(img),wrap_around_axis_0=False, wrap_around_axis_1=False, wrap_around_axis_2=False)
            label_normalized = normalize_angle(label)
            batch_labels.append(np.expand_dims(label_normalized, axis=-1))
            
        return np.array(batch_imgs) ,np.array(batch_labels)

wrapped_image_dir = 'PU'



def make_pair(start, end):
    pairs = []
    for i in range (start, end):
        pairs.append ([(wrapped_image_dir + '/wraped_' + str(i+1)+'.mat'), (wrapped_image_dir + '/un_wraped_' + str(i+1)+'.mat')])
    
    return pairs
    
    

from random import shuffle
train_pair = make_pair(1, 5000) #25% train 
val_pair = make_pair(5001, 10001) # 25 % val
test_pair = make_pair(10001, 15000)#50% testing 

from random import sample, choice
temp = choice(train_pair)
print (temp[0], "GAB", temp[1])
tempx = choice(val_pair)
print (tempx[0], "GAB", tempx[1])

import matplotlib.pyplot as plt
img = scipy.io.loadmat(temp[0])['wrap']
mask_x= unwrap(np.angle(img),wrap_around_axis_0=False, wrap_around_axis_1=False, wrap_around_axis_2=False)
plt.figure(figsize=(10,10))
plt.subplot(121)
plt.imshow(normalize_angle(np.angle(img)), cmap='gray')
plt.subplot(122)
plt.imshow(normalize_angle(mask_x), cmap='gray')

x = normalize_angle(np.angle(img))
print (np.min (x), np.max(x))
x = normalize_angle(mask_x)
print (np.min (x), np.max(x))

import matplotlib.pyplot as plt
img = scipy.io.loadmat(tempx[0])['wrap']
mask_x= unwrap(np.angle(img),wrap_around_axis_0=False, wrap_around_axis_1=False, wrap_around_axis_2=False)
plt.figure(figsize=(10,10))
plt.subplot(121)
plt.imshow(np.angle(img), cmap='jet')
plt.subplot(122)
plt.imshow(mask_x, cmap='jet')

class_map=1
train_generator = DataGenerator(train_pair,class_map,batch_size=20, dim=(256,256,1) ,shuffle=True)
train_steps = train_generator.__len__()
train_steps

val_generator = DataGenerator(val_pair, class_map, batch_size=20, dim=(256,256,1) ,shuffle=True)
val_steps = val_generator.__len__()
val_steps

test_generator= DataGenerator(test_pair, class_map, batch_size=20, dim=(256,256,1) ,shuffle=True)
test_steps = test_generator.__len__()
test_steps

X,y = train_generator.__getitem__(1)
y.shape
X.shape

window_size=256
n_CLASSES=1
##############################

unet_model = model.r2_unet(window_size, window_size, n_CLASSES,data_format='channels_last')
unet_model.summary()

unet_model.fit_generator(train_generator , steps_per_epoch=train_steps ,epochs=20,validation_data=val_generator,validation_steps=val_steps)
unet_model.save_weights('PhU_r2_unet_weights.h5')

##############################
unet_model = model.unet(window_size, window_size, n_CLASSES,data_format='channels_last')
unet_model.summary()

unet_model.fit_generator(train_generator , steps_per_epoch=train_steps ,epochs=20,validation_data=val_generator,validation_steps=val_steps)
unet_model.save_weights('PhU_unet_weights.h5')

##############################
unet_model = model.r_unet(window_size, window_size, n_CLASSES,data_format='channels_last')
unet_model.summary()

unet_model.fit_generator(train_generator , steps_per_epoch=train_steps ,epochs=20,validation_data=val_generator,validation_steps=val_steps)
unet_model.save_weights('PhU_r_unet_weights.h5')